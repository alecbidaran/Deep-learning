{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPK5ZsBFfsVitNVx4186W/I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alecbidaran/Deep-learning/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncAMCmbXpKoe"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCTaeIo_shdK"
      },
      "source": [
        "\n",
        "# Define global constants to be used in this notebook\n",
        "BATCH_SIZE=128\n",
        "LATENT_DIM=2\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq0TNTW_sjdC"
      },
      "source": [
        "def map_image(image, label):\n",
        "  '''returns a normalized and reshaped tensor from a given image'''\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = image / 255.0\n",
        "  image = tf.reshape(image, shape=(28, 28, 1,))\n",
        "  \n",
        "  return image\n",
        "\n",
        "\n",
        "def get_dataset(map_fn, is_validation=False):\n",
        "  '''Loads and prepares the mnist dataset from TFDS.'''\n",
        "  if is_validation:\n",
        "    split_name = \"test\"\n",
        "  else:\n",
        "    split_name = \"train\"\n",
        "\n",
        "  dataset = tfds.load('mnist', as_supervised=True, split=split_name)\n",
        "  dataset = dataset.map(map_fn)\n",
        "  \n",
        "  if is_validation:\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "  else:\n",
        "    dataset = dataset.shuffle(1024).batch(BATCH_SIZE)\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0geqWexsreR"
      },
      "source": [
        "train_dataset = get_dataset(map_image)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-2P7Tq9suOJ"
      },
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self,inputs):\n",
        "    mu,sigma=inputs\n",
        "    batch=tf.shape(mu)[0]\n",
        "    dim=tf.shape(mu)[1]\n",
        "    epsilon=tf.keras.backend.random_normal(shape=(batch,dim))\n",
        "    return mu+tf.exp(sigma*0.5)*epsilon"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DYIe8egtuqO"
      },
      "source": [
        "def encoder_layers(inputs, latent_dim):\n",
        "  \"\"\"Defines the encoder's layers.\n",
        "  Args:\n",
        "    inputs -- batch from the dataset\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "\n",
        "  Returns:\n",
        "    mu -- learned mean\n",
        "    sigma -- learned standard deviation\n",
        "    batch_2.shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "\n",
        "  # add the Conv2D layers followed by BatchNormalization\n",
        "  x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)\n",
        "\n",
        "  # assign to a different variable so you can extract the shape later\n",
        "  batch_2 = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # flatten the features and feed into the Dense network\n",
        "  x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_2)\n",
        "\n",
        "  # we arbitrarily used 20 units here but feel free to change and see what results you get\n",
        "  x = tf.keras.layers.Dense(20, activation='relu', name=\"encode_dense\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # add output Dense networks for mu and sigma, units equal to the declared latent_dim.\n",
        "  mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "  sigma = tf.keras.layers.Dense(latent_dim, name ='latent_sigma')(x)\n",
        "\n",
        "  return mu, sigma, batch_2.shape"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwmTNEGCvfZD"
      },
      "source": [
        "def encoder_model(latent_dim, input_shape):\n",
        "  \"\"\"Defines the encoder model with the Sampling layer\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    model -- the encoder model\n",
        "    conv_shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "\n",
        "  # declare the inputs tensor with the given shape\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  # get the output of the encoder_layers() function\n",
        "  mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=LATENT_DIM)\n",
        "\n",
        "  # feed mu and sigma to the Sampling layer\n",
        "  z = Sampling()((mu, sigma))\n",
        "\n",
        "  # build the whole encoder model\n",
        "  model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "  return model, conv_shape"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "namMshBBwqUv"
      },
      "source": [
        "def decoder_layers(inputs, conv_shape):\n",
        "  \"\"\"Defines the decoder layers.\n",
        "  Args:\n",
        "    inputs -- output of the encoder \n",
        "    conv_shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    tensor containing the decoded output\n",
        "  \"\"\"\n",
        "\n",
        "  # feed to a Dense network with units computed from the conv_shape dimensions\n",
        "  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "  x = tf.keras.layers.Dense(units, activation = 'relu', name=\"decode_dense1\")(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  # reshape output using the conv_shape dimensions\n",
        "  x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n",
        "\n",
        "  # upsample the features back to the original dimensions\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_2\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_3\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid', name=\"decode_final\")(x)\n",
        "  \n",
        "  return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2xWCOOhxXP_"
      },
      "source": [
        "def decoder_model(latent_dim, conv_shape):\n",
        "  \"\"\"Defines the decoder model.\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    conv_shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    model -- the decoder model\n",
        "  \"\"\"\n",
        "\n",
        "  # set the inputs to the shape of the latent space\n",
        "  inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "\n",
        "  # get the output of the decoder layers\n",
        "  outputs = decoder_layers(inputs, conv_shape)\n",
        "\n",
        "  # declare the inputs and outputs of the model\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5-T3h8hx7wm"
      },
      "source": [
        "def kl_reconstruction_loss(inputs,outputs,mu,sigma):\n",
        "  kl=1+sigma*tf.square(mu)-tf.math.exp(sigma)\n",
        "  return tf.reduce_mean(kl)*-0.5"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj-NMLRpyZhN"
      },
      "source": [
        "def vae_model(encoder, decoder, input_shape):\n",
        "  \"\"\"Defines the VAE model\n",
        "  Args:\n",
        "    encoder -- the encoder model\n",
        "    decoder -- the decoder model\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    the complete VAE model\n",
        "  \"\"\"\n",
        "\n",
        "  # set the inputs\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  # get mu, sigma, and z from the encoder output\n",
        "  mu, sigma, z = encoder(inputs)\n",
        "  \n",
        "  # get reconstructed output from the decoder\n",
        "  reconstructed = decoder(z)\n",
        "\n",
        "  # define the inputs and outputs of the VAE\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "\n",
        "  # add the KL loss\n",
        "  loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "  model.add_loss(loss)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pmpHEFAzHP-"
      },
      "source": [
        "def get_models(input_shape, latent_dim):\n",
        "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "  encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "  decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "  vae = vae_model(encoder, decoder,input_shape)\n",
        "  return encoder, decoder, vae"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CGP_gTKzW-E"
      },
      "source": [
        "encoder, decoder, vae = get_models(input_shape=(28,28,1,), latent_dim=LATENT_DIM)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "bce_loss = tf.keras.losses.BinaryCrossentropy()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzL-1NE5zaAO"
      },
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "  \"\"\"Helper function to plot our 16 images\n",
        "\n",
        "  Args:\n",
        "\n",
        "  model -- the decoder model\n",
        "  epoch -- current epoch number during training\n",
        "  step -- current step number during training\n",
        "  test_input -- random tensor with shape (16, LATENT_DIM)\n",
        "  \"\"\"\n",
        "\n",
        "  # generate images from the test input\n",
        "  predictions = model.predict(test_input)\n",
        "\n",
        "  # plot the results\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "  plt.show()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "IyQKzzdq3gLO",
        "outputId": "6b431196-91fe-49b8-adad-62f0a0497b80"
      },
      "source": [
        "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
        "\n",
        "# number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# initialize the helper function to display outputs from an untrained model\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for step,x_batch_train in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      reconstructed=vae(x_batch_train)\n",
        "      flattened_input=tf.reshape(x_batch_train,shape=[-1])\n",
        "      flattened_output=tf.reshape(reconstructed,shape=[-1])\n",
        "      loss=bce_loss(flattened_input,flattened_output)*28*28\n",
        "      loss+=sum(vae.losses)\n",
        "    grads=tape.gradient(loss,vae.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads,vae.trainable_variables))\n",
        "    loss_metric(loss)\n",
        "\n",
        "    # display outputs every 100 steps\n",
        "    if step % 100 == 0:\n",
        "      display.clear_output(wait=False)    \n",
        "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
        "      print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 99 step: 400 mean loss = nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDaAbpg04-e3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}